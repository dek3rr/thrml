{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# Parallel Tempering with THRML-Boost\n",
    "\n",
    "Standard block Gibbs sampling is fast, but it has a blind spot: **multimodal distributions**. When the energy landscape has multiple deep wells separated by high barriers, the sampler gets trapped in whichever mode it finds first and never escapes.\n",
    "\n",
    "This notebook shows the problem concretely, then demonstrates how parallel tempering fixes it — and how THRML-Boost runs all chains simultaneously via `jax.vmap` so you pay no extra compile cost for additional chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e0a2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b4d0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thrml_boost.block_management import Block\n",
    "from thrml_boost.block_sampling import SamplingSchedule, sample_states\n",
    "from thrml_boost.models.ising import IsingEBM, IsingSamplingProgram, hinton_init\n",
    "from thrml_boost.pgm import SpinNode\n",
    "from thrml_boost.tempering import parallel_tempering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2689b53",
   "metadata": {},
   "source": [
    "## Setup: a strongly ferromagnetic Ising model\n",
    "\n",
    "We use a 10×10 periodic Ising grid with strong ferromagnetic coupling and no bias. This model has two symmetric ground states: all spins up and all spins down, separated by a large energy barrier. It's a clean testbed for mixing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10e09ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "side = 10\n",
    "G = nx.grid_2d_graph(side, side, periodic=True)\n",
    "coord_to_node = {coord: SpinNode() for coord in G.nodes()}\n",
    "nx.relabel_nodes(G, coord_to_node, copy=False)\n",
    "\n",
    "nodes = list(G.nodes())\n",
    "edges = list(G.edges())\n",
    "\n",
    "beta_target = 2.0\n",
    "biases = jnp.zeros(len(nodes))\n",
    "weights = jnp.ones(len(edges)) * 1.0  # ferromagnetic\n",
    "\n",
    "# Two-color for block Gibbs\n",
    "coloring = nx.bipartite.color(G)\n",
    "color0 = [n for n, c in coloring.items() if c == 0]\n",
    "color1 = [n for n, c in coloring.items() if c == 1]\n",
    "free_blocks = [Block(color0), Block(color1)]\n",
    "\n",
    "key = jax.random.key(42)\n",
    "print(f\"Nodes: {len(nodes)}, Edges: {len(edges)}, Block sizes: {len(color0)}, {len(color1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e0e668",
   "metadata": {},
   "source": [
    "## The problem: Gibbs gets stuck\n",
    "\n",
    "We track the **mean magnetization** $\\langle s \\rangle = \\frac{1}{N}\\sum_i s_i$ over time. A sampler that mixes well should oscillate between $+1$ (all up) and $-1$ (all down). A stuck sampler stays near one value indefinitely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e337b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm = IsingEBM(nodes, edges, biases, weights, jnp.array(beta_target))\n",
    "program = IsingSamplingProgram(ebm, free_blocks, clamped_blocks=[])\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "init_state = hinton_init(subkey, ebm, free_blocks, ())\n",
    "\n",
    "schedule = SamplingSchedule(n_warmup=50, n_samples=400, steps_per_sample=2)\n",
    "key, subkey = jax.random.split(key)\n",
    "gibbs_samples = sample_states(subkey, program, schedule, init_state, [], [Block(nodes)])\n",
    "\n",
    "spins = 2 * gibbs_samples[0].astype(jnp.int8) - 1\n",
    "mag_gibbs = jnp.mean(spins, axis=-1)\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(mag_gibbs, lw=0.8, color=\"steelblue\")\n",
    "plt.axhline(0, color=\"k\", lw=0.5, linestyle=\"--\")\n",
    "plt.fill_between(range(len(mag_gibbs)), -1, 1, alpha=0.03, color=\"gray\")\n",
    "plt.ylim(-1.1, 1.1)\n",
    "plt.xlabel(\"Sample\")\n",
    "plt.ylabel(\"Mean magnetization\")\n",
    "plt.title(f\"Standard Gibbs at β={beta_target} — trapped in one mode\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2e929a",
   "metadata": {},
   "source": [
    "## The fix: parallel tempering\n",
    "\n",
    "Parallel tempering runs multiple chains at different temperatures simultaneously:\n",
    "\n",
    "- **Hot chains** (small β) — energy barriers are flattened, the chain explores freely\n",
    "- **Cold chains** (large β) — accurate samples from the target distribution\n",
    "- Every round, adjacent chains **propose a swap**. If accepted, the cold chain receives a configuration the hot chain found on the other side of a barrier.\n",
    "\n",
    "But this only works if the temperature ladder is tuned correctly. If adjacent chains are too different, swaps are almost never accepted and the chains don't communicate. Let's see what that looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f4e0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A naive beta ladder — large gap at the bottom, crowded at the top\n",
    "betas_naive = [0.1, 0.3, 0.7, 1.2, beta_target]\n",
    "n_chains = len(betas_naive)\n",
    "\n",
    "ebms_naive = [IsingEBM(nodes, edges, biases, weights, jnp.array(b)) for b in betas_naive]\n",
    "programs_naive = [IsingSamplingProgram(e, free_blocks, []) for e in ebms_naive]\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "init = hinton_init(subkey, ebms_naive[0], free_blocks, ())\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def one_round_naive(key, states):\n",
    "    return parallel_tempering(\n",
    "        key,\n",
    "        ebms_naive,\n",
    "        programs_naive,\n",
    "        states,\n",
    "        [],\n",
    "        n_rounds=1,\n",
    "        gibbs_steps_per_round=2,\n",
    "    )\n",
    "\n",
    "\n",
    "mag_naive = []\n",
    "states_naive = [init] * n_chains\n",
    "key, subkey = jax.random.split(key)\n",
    "for k in jax.random.split(subkey, 400):\n",
    "    states_naive, _, stats_naive = one_round_naive(k, states_naive)\n",
    "    cold = jnp.concatenate(states_naive[-1])\n",
    "    mag_naive.append(float(jnp.mean(2 * cold.astype(jnp.int8) - 1)))\n",
    "\n",
    "print(\"Swap acceptance rates with naive ladder:\")\n",
    "for i, r in enumerate(stats_naive[\"acceptance_rate\"]):\n",
    "    flag = \"✓\" if 0.2 <= float(r) <= 0.5 else \"✗\"\n",
    "    print(f\"  {flag} β={betas_naive[i]:.1f} ↔ β={betas_naive[i + 1]:.1f}: {float(r):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9831e131",
   "metadata": {},
   "source": [
    "The large gap between β=0.1 and β=0.3 gives near-zero swap acceptance — those chains never communicate, so the cold chain still can't escape its mode.\n",
    "\n",
    "## Tuning the temperature ladder\n",
    "\n",
    "We need acceptance rates in the **20–50% range on every pair**. The iterative refinement below starts from a coarse geometric ladder and repeatedly inserts a new temperature at the geometric midpoint of the worst-performing pair until convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62c3bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_acceptance(betas, key, n_rounds=300):\n",
    "    n = len(betas)\n",
    "    ebms_t = [IsingEBM(nodes, edges, biases, weights, jnp.array(b)) for b in betas]\n",
    "    progs_t = [IsingSamplingProgram(e, free_blocks, []) for e in ebms_t]\n",
    "    key, subkey = jax.random.split(key)\n",
    "    init_t = hinton_init(subkey, ebms_t[0], free_blocks, ())\n",
    "\n",
    "    @jax.jit\n",
    "    def _run(key, states):\n",
    "        return parallel_tempering(key, ebms_t, progs_t, states, [], n_rounds=n_rounds, gibbs_steps_per_round=2)\n",
    "\n",
    "    key, subkey = jax.random.split(key)\n",
    "    _, _, stats = _run(subkey, [init_t] * n)\n",
    "    return np.array(stats[\"acceptance_rate\"])\n",
    "\n",
    "\n",
    "def refine_ladder(betas, key, min_rate=0.2, max_iters=8):\n",
    "    betas = sorted(list(betas))\n",
    "    for i in range(max_iters):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        rates = run_acceptance(betas, subkey)\n",
    "        worst = int(np.argmin(rates))\n",
    "        worst_rate = rates[worst]\n",
    "        print(\n",
    "            f\"  Iter {i + 1}: {len(betas)} chains, \"\n",
    "            f\"min rate = {worst_rate:.1%} at β={betas[worst]:.3f}↔{betas[worst + 1]:.3f}\"\n",
    "        )\n",
    "        if worst_rate >= min_rate:\n",
    "            print(f\"  All pairs above {min_rate:.0%} — converged.\")\n",
    "            break\n",
    "        new_beta = float(np.sqrt(betas[worst] * betas[worst + 1]))\n",
    "        betas.insert(worst + 1, new_beta)\n",
    "    return betas, rates\n",
    "\n",
    "\n",
    "print(\"Refining from a 5-chain geometric ladder...\")\n",
    "start_betas = np.geomspace(0.1, beta_target, 5).tolist()\n",
    "key, subkey = jax.random.split(key)\n",
    "refined_betas, _ = refine_ladder(start_betas, subkey)\n",
    "print(f\"\\nFinal ladder: {[f'{b:.3f}' for b in refined_betas]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fa1c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot final acceptance rates\n",
    "key, subkey = jax.random.split(key)\n",
    "final_rates = run_acceptance(refined_betas, subkey)\n",
    "pair_labels = [f\"{refined_betas[i]:.2f}↔{refined_betas[i + 1]:.2f}\" for i in range(len(refined_betas) - 1)]\n",
    "bar_colors = [\"green\" if 0.2 <= r <= 0.5 else \"orange\" if r > 0.5 else \"red\" for r in final_rates]\n",
    "\n",
    "plt.figure(figsize=(9, 3))\n",
    "plt.bar(pair_labels, final_rates, color=bar_colors, edgecolor=\"white\")\n",
    "plt.axhline(0.2, color=\"k\", lw=1, linestyle=\"--\", label=\"20% lower target\")\n",
    "plt.axhline(0.5, color=\"k\", lw=1, linestyle=\":\", label=\"50% upper target\")\n",
    "plt.ylabel(\"Acceptance rate\")\n",
    "plt.title(f\"Refined ladder — {len(refined_betas)} chains\")\n",
    "plt.xticks(rotation=25, ha=\"right\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec69fcf",
   "metadata": {},
   "source": [
    "## Comparison: naive vs tuned ladder\n",
    "\n",
    "Now we can see the real benefit. With a well-tuned ladder, the cold chain mixes freely between both modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c63f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PT with refined betas\n",
    "ebms_refined = [IsingEBM(nodes, edges, biases, weights, jnp.array(b)) for b in refined_betas]\n",
    "programs_refined = [IsingSamplingProgram(e, free_blocks, []) for e in ebms_refined]\n",
    "n_ref = len(refined_betas)\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "init_ref = hinton_init(subkey, ebms_refined[0], free_blocks, ())\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def one_round_refined(key, states):\n",
    "    return parallel_tempering(\n",
    "        key,\n",
    "        ebms_refined,\n",
    "        programs_refined,\n",
    "        states,\n",
    "        [],\n",
    "        n_rounds=1,\n",
    "        gibbs_steps_per_round=2,\n",
    "    )\n",
    "\n",
    "\n",
    "mag_refined = []\n",
    "states_ref = [init_ref] * n_ref\n",
    "key, subkey = jax.random.split(key)\n",
    "for k in jax.random.split(subkey, 400):\n",
    "    states_ref, _, _ = one_round_refined(k, states_ref)\n",
    "    cold = jnp.concatenate(states_ref[-1])\n",
    "    mag_refined.append(float(jnp.mean(2 * cold.astype(jnp.int8) - 1)))\n",
    "\n",
    "# Three-way comparison\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 7), sharex=True)\n",
    "\n",
    "for ax, mag, color, title in zip(\n",
    "    axes,\n",
    "    [mag_gibbs, mag_naive, mag_refined],\n",
    "    [\"steelblue\", \"orangered\", \"green\"],\n",
    "    [\n",
    "        \"Standard Gibbs — trapped\",\n",
    "        f\"PT with naive betas {[round(b, 1) for b in betas_naive]} — still trapped\",\n",
    "        f\"PT with refined betas ({len(refined_betas)} chains) — mixing between modes\",\n",
    "    ],\n",
    "):\n",
    "    ax.plot(mag, lw=0.8, color=color)\n",
    "    ax.axhline(0, color=\"k\", lw=0.5, linestyle=\"--\")\n",
    "    ax.set_ylim(-1.1, 1.1)\n",
    "    ax.set_ylabel(\"Magnetization\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "axes[-1].set_xlabel(\"Round\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3fbc09",
   "metadata": {},
   "source": [
    "## Performance: compile time stays flat with more chains\n",
    "\n",
    "In the original thrml, more chains meant longer compile time because each chain was unrolled separately into the XLA graph. With `jax.vmap` in THRML-Boost, compile time is essentially constant — you only pay in runtime, which scales linearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dacde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_counts = [2, 4, 8, 16]\n",
    "compile_times = []\n",
    "\n",
    "for n in chain_counts:\n",
    "    betas_n = np.geomspace(0.1, beta_target, n).tolist()\n",
    "    ebms_n = [IsingEBM(nodes, edges, biases, weights, jnp.array(b)) for b in betas_n]\n",
    "    progs_n = [IsingSamplingProgram(e, free_blocks, []) for e in ebms_n]\n",
    "    key, subkey = jax.random.split(key)\n",
    "    init_n = hinton_init(subkey, ebms_n[0], free_blocks, ())\n",
    "\n",
    "    @jax.jit\n",
    "    def _run(key, states):\n",
    "        return parallel_tempering(key, ebms_n, progs_n, states, [], n_rounds=5, gibbs_steps_per_round=1)\n",
    "\n",
    "    key, subkey = jax.random.split(key)\n",
    "    t0 = time.time()\n",
    "    result = _run(subkey, [init_n] * n)\n",
    "    jax.block_until_ready(result[0])\n",
    "    compile_times.append(time.time() - t0)\n",
    "    print(f\"  {n:2d} chains: {compile_times[-1]:.2f}s\")\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(chain_counts, compile_times, \"o-\", color=\"steelblue\", label=\"THRML-Boost (vmap)\")\n",
    "plt.xlabel(\"Number of chains\")\n",
    "plt.ylabel(\"First-call time (compile + run, s)\")\n",
    "plt.title(\"Compile time is flat across chain counts\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Parallel tempering in THRML-Boost:\n",
    "\n",
    "- Escapes multimodal traps that standard Gibbs cannot\n",
    "- All chains run in a single `jax.vmap` kernel — no compile cost for extra chains\n",
    "- Swap statistics guide temperature ladder tuning\n",
    "- The iterative refinement loop above handles problem-specific structure automatically\n",
    "\n",
    "```python\n",
    "final_states, sampler_states, stats = parallel_tempering(\n",
    "    key,\n",
    "    ebms,            # one IsingEBM per temperature\n",
    "    programs,        # one IsingSamplingProgram per temperature\n",
    "    init_states,     # one initial state per temperature\n",
    "    clamp_state=[],\n",
    "    n_rounds=500,\n",
    "    gibbs_steps_per_round=2,\n",
    ")\n",
    "\n",
    "# Check your ladder\n",
    "print(stats['acceptance_rate'])   # aim for 0.2–0.5 on every pair\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
